{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A $\\bf{Race~Condition}$ in the context of parallel computation is the situation where the resulting value depends on the order of the computational operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of this is:\n",
    "\n",
    "|  Thread 1    |   Thread 2   |  | Integer value |\n",
    "\n",
    "|              |              |  |       0       |\n",
    "\n",
    "| read value   |              |<-|       0       |\n",
    "\n",
    "|increase value|              |  |       0       |\n",
    "\n",
    "|write back    |              |->|       1       |\n",
    "\n",
    "|              |  read value  |<-|       1       |\n",
    "\n",
    "|              |increase value|  |       1       |\n",
    "\n",
    "|              | write back   |->|       2       |\n",
    "\n",
    "And the result is 2; but in this case:\n",
    "\n",
    "|  Thread 1    |  Thread 2    |  | Integer value |\n",
    "|              |              |  |       0       |\n",
    "|  read value  |              |<-|       0       |\n",
    "|              | read value   |<-|       0       |\n",
    "|increase value|              |  |       0       |\n",
    "|              |increase value|  |       0       |\n",
    "| write back   |              |->|       1       |\n",
    "|              |  write back  |->|       1       |\n",
    "\n",
    "the result is 1.\n",
    "\n",
    "(note: for some reason, latex tables don't work here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\bf MapReduce}$ is a programming model for processing and generating large data sets with a parallel, distributed algorithm on a cluster.\n",
    "It is also a framework for processing parallelizable problems across huge data sets, using a large number of computers (nodes); clusters or grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf{The~difference~between~MapReduce~and~Hadoop}$ is that Hadoop is the Distributed File System that stores the data. The Hadoop framework is implemented in Java, and provides an API to MapReduce that allows you to write your map and reduce functions in languages other than Java."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is based on the MapReduce programming paradigm. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of a string here is 10\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def length(s):\n",
    "    return len(s)\n",
    "strings = [\"str1\", \"string2\", \"w261\", \"Machine learning at Scale\"]\n",
    "S = map(length, strings)\n",
    "\n",
    "print \"Average length of a string here is\", (reduce(lambda x, y: x+y, S)/len(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.1 Sorting in MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate the set of random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/hadoop/Notebooks/Users/Nina'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘SortData’: File exists\n",
      "mkdir: cannot create directory ‘SortCode’: File exists\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%mkdir SortData\n",
    "%mkdir SortCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SortCode/hw2_1_generate_numbers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SortCode/hw2_1_generate_numbers.py\n",
    "#!/home/hadoop/anaconda2/bin/python\n",
    "\n",
    "import random\n",
    "random.seed(10001)\n",
    "N = 10000\n",
    "for n in range(N):\n",
    "    print random.randint(0,N),\"\\t\",\"NA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we redirect the output of this file to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 SortData/hw2_1_NumbersDataSet.txt\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x SortCode/hw2_1_generate_numbers.py;\n",
    "!./SortCode/hw2_1_generate_numbers.py > SortData/hw2_1_NumbersDataSet.txt\n",
    "!wc -l SortData/hw2_1_NumbersDataSet.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we check the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3563 \tNA\r\n",
      "8248 \tNA\r\n",
      "1194 \tNA\r\n",
      "5810 \tNA\r\n",
      "6439 \tNA\r\n",
      "5566 \tNA\r\n",
      "1338 \tNA\r\n",
      "9680 \tNA\r\n",
      "211 \tNA\r\n",
      "3053 \tNA\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!cat SortData/hw2_1_NumbersDataSet.txt | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a directory for this in hdfs and put the file there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `Sort': File exists\n",
      "put: `Sort/hw2_1_NumbersDataSet.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -mkdir Sort\n",
    "! hdfs dfs -put SortData/hw2_1_NumbersDataSet.txt Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘SortOutput’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "% mkdir SortOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%hdfs` not found.\n"
     ]
    }
   ],
   "source": [
    "% hdfs dfs -put SortOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3563 \tNA\r\n",
      "8248 \tNA\r\n",
      "1194 \tNA\r\n",
      "5810 \tNA\r\n",
      "6439 \tNA\r\n",
      "5566 \tNA\r\n",
      "1338 \tNA\r\n",
      "9680 \tNA\r\n",
      "211 \tNA\r\n",
      "3053 \tNA\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat Sort/hw2_1_NumbersDataSet.txt | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we write a file that reads in this file (note: this code is inspired by the posted notebook 'MIDS-W261-Partial-Total-Secondary-Sorts'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SortCode/identityFunction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SortCode/identityFunction.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    key,value = line.split(\"\\t\", 1)\n",
    "    print '%s\\t%s' % (key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let it run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/03 02:19:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted Sort/Output\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar] /tmp/streamjob7752606040531101698.jar tmpDir=null\n",
      "16/06/03 02:19:49 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-7-251.us-west-1.compute.internal/172.31.7.251:8032\n",
      "16/06/03 02:19:49 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-7-251.us-west-1.compute.internal/172.31.7.251:8032\n",
      "16/06/03 02:19:49 INFO metrics.MetricsSaver: MetricsConfigRecord disabledInCluster: false instanceEngineCycleSec: 60 clusterEngineCycleSec: 60 disableClusterEngine: true maxMemoryMb: 3072 maxInstanceCount: 500 lastModified: 1464726748890 \n",
      "16/06/03 02:19:49 INFO metrics.MetricsSaver: Created MetricsSaver j-ZAC3GQDMC0E6:i-610a91d4:RunJar:06792 period:60 /mnt/var/em/raw/i-610a91d4_20160603_RunJar_06792_raw.bin\n",
      "16/06/03 02:19:50 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "16/06/03 02:19:50 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 426d94a07125cf9447bb0c2b336cf10b4c254375]\n",
      "16/06/03 02:19:50 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/03 02:19:50 INFO mapreduce.JobSubmitter: number of splits:16\n",
      "16/06/03 02:19:50 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/06/03 02:19:50 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/06/03 02:19:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464726740139_0019\n",
      "16/06/03 02:19:50 INFO impl.YarnClientImpl: Submitted application application_1464726740139_0019\n",
      "16/06/03 02:19:50 INFO mapreduce.Job: The url to track the job: http://ip-172-31-7-251.us-west-1.compute.internal:20888/proxy/application_1464726740139_0019/\n",
      "16/06/03 02:19:50 INFO mapreduce.Job: Running job: job_1464726740139_0019\n",
      "16/06/03 02:19:57 INFO mapreduce.Job: Job job_1464726740139_0019 running in uber mode : false\n",
      "16/06/03 02:19:57 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/03 02:20:07 INFO mapreduce.Job:  map 6% reduce 0%\n",
      "16/06/03 02:20:10 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "16/06/03 02:20:13 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "16/06/03 02:20:15 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "16/06/03 02:20:19 INFO mapreduce.Job:  map 69% reduce 0%\n",
      "16/06/03 02:20:20 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "16/06/03 02:20:22 INFO mapreduce.Job:  map 81% reduce 0%\n",
      "16/06/03 02:20:23 INFO mapreduce.Job:  map 88% reduce 0%\n",
      "16/06/03 02:20:24 INFO mapreduce.Job:  map 94% reduce 0%\n",
      "16/06/03 02:20:27 INFO mapreduce.Job:  map 100% reduce 4%\n",
      "16/06/03 02:20:29 INFO mapreduce.Job:  map 100% reduce 43%\n",
      "16/06/03 02:20:30 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "16/06/03 02:20:34 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/03 02:20:34 INFO mapreduce.Job: Job job_1464726740139_0019 completed successfully\n",
      "16/06/03 02:20:35 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=34821\n",
      "\t\tFILE: Number of bytes written=3034481\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=695868\n",
      "\t\tHDFS: Number of bytes written=88988\n",
      "\t\tHDFS: Number of read operations=69\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=14\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=16\n",
      "\t\tLaunched reduce tasks=7\n",
      "\t\tData-local map tasks=8\n",
      "\t\tRack-local map tasks=8\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6989760\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5937300\n",
      "\t\tTotal time spent by all map tasks (ms)=155328\n",
      "\t\tTotal time spent by all reduce tasks (ms)=65970\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=155328\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=65970\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=223672320\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=189993600\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=88988\n",
      "\t\tMap output materialized bytes=58002\n",
      "\t\tInput split bytes=2368\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6302\n",
      "\t\tReduce shuffle bytes=58002\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =112\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=112\n",
      "\t\tGC time elapsed (ms)=2812\n",
      "\t\tCPU time spent (ms)=31560\n",
      "\t\tPhysical memory (bytes) snapshot=8635715584\n",
      "\t\tVirtual memory (bytes) snapshot=56009691136\n",
      "\t\tTotal committed heap usage (bytes)=10237247488\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=693500\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=88988\n",
      "16/06/03 02:20:35 INFO streaming.StreamJob: Output directory: Sort/Output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r Sort/Output\n",
    "!hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar \\\n",
    "   -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "   -D mapred.text.key.comparator.options=-nr \\\n",
    "   -mapper /bin/cat \\\n",
    "   -reducer /bin/cat \\\n",
    "   -input Sort/hw2_1_NumbersDataSet.txt  -output Sort/Output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, when we look at the output directory, we see all the sorted numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\r\n",
      "-rw-r--r--   1 hadoop hadoop          0 2016-06-03 02:20 Sort/Output/_SUCCESS\r\n",
      "-rw-r--r--   1 hadoop hadoop      12400 2016-06-03 02:20 Sort/Output/part-00000\r\n",
      "-rw-r--r--   1 hadoop hadoop      13112 2016-06-03 02:20 Sort/Output/part-00001\r\n",
      "-rw-r--r--   1 hadoop hadoop      12759 2016-06-03 02:20 Sort/Output/part-00002\r\n",
      "-rw-r--r--   1 hadoop hadoop      12832 2016-06-03 02:20 Sort/Output/part-00003\r\n",
      "-rw-r--r--   1 hadoop hadoop      12613 2016-06-03 02:20 Sort/Output/part-00004\r\n",
      "-rw-r--r--   1 hadoop hadoop      12729 2016-06-03 02:20 Sort/Output/part-00005\r\n",
      "-rw-r--r--   1 hadoop hadoop      12543 2016-06-03 02:20 Sort/Output/part-00006\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls Sort/Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we view the highest values are the top 10 lines in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9991 \tNA\r\n",
      "9991 \tNA\r\n",
      "9991 \tNA\r\n",
      "9991 \tNA\r\n",
      "9984 \tNA\r\n",
      "9984 \tNA\r\n",
      "9970 \tNA\r\n",
      "9970 \tNA\r\n",
      "9949 \tNA\r\n",
      "9942 \tNA\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat Sort/Output/part-00000 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the lowest with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 \tNA\r\n",
      "58 \tNA\r\n",
      "51 \tNA\r\n",
      "44 \tNA\r\n",
      "44 \tNA\r\n",
      "44 \tNA\r\n",
      "37 \tNA\r\n",
      "23 \tNA\r\n",
      "16 \tNA\r\n",
      "5 \tNA\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat Sort/Output/part-00004 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW 2.2 Wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\r\n"
     ]
    }
   ],
   "source": [
    "!grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we set the mapper and reducer for this file. In the mapper, start by printing the words and their count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following mapper-reducer files are borrowed from the master solution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "print string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    #line = line.strip().split('\\t', 2)[-1]\n",
    "    for c in string.punctuation:\n",
    "        line= line.replace(c,\"\")\n",
    "    line = line.strip().split('\\t', 2)[-1]\n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    line = regex.sub(' ', line.lower())\n",
    "    line = re.sub( '\\s+', ' ', line )\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        if len(word) > 1:  #drop single character words\n",
    "            print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # save count\n",
    "             print '%s\\t%d' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to save the last word count if needed!\n",
    "if current_word == word:    \n",
    "     print '%s\\t%d' % (current_word, current_count)\n",
    "    \n",
    "# found count for word assistance\n",
    "#findword = 'assistance'\n",
    "#print '%s\\t%d' %(findword, wordcount[findword] if findword in wordcount else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are you\\n\")\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in sys.stdin:\n",
    "    for word in line.split():\n",
    "        word = word.lower()\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "        if word == \"debt\":\n",
    "            sys.stderr.write(\"reporter:counter:EDA Counters,Calls,1\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "#token_occurence = sorted(token_occurence, key=lambda key: token_occurence[val])\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/Users/ninakuklisova/miniconda2/envs/jupi/bin/python\n",
    "## mapper.py\n",
    "## Author: Nina Kuklisova\n",
    "## Description: mapper code for HW2.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "#count = 0\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[2] \n",
    "findwords = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "\n",
    "    for line in myfile.readlines():\n",
    "        count = [0]*len(findwords)\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            # if the word is found in the list of words, increase its count by 1\n",
    "            if word in findwords:\n",
    "                count[findwords.index[word]] = count[findwords.index[word]] + 1\n",
    "        print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/hadoop/Notebooks/Users/Nina'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/Users/ninakuklisova/miniconda2/envs/jupi/bin/python\n",
    "\n",
    "import sys\n",
    "print sys.argv\n",
    "sum = 0\n",
    "filename = sys.argv[2] #\"enronemail_1h.txt\"\n",
    "#findword = sys.argv[1]\n",
    "\n",
    "with open (filename, 'rb') as myfile:\n",
    "    count = 0\n",
    "    # take the words in the first line\n",
    "    for line in myfile:\n",
    "        if count==0:\n",
    "            findwords = line.split()\n",
    "            word_count = len(findwords) *[0]\n",
    "        else:\n",
    "            counts = line.split()\n",
    "            for i in range(len(findwords)):\n",
    "                word_count[i]+=int(counts[i])\n",
    "\n",
    "# find the most popular token:\n",
    "token_occurence = {}\n",
    "for i in range(len(findwords)):\n",
    "    (key, val) = (findwords[i], word_count[i])\n",
    "    token_occurence[int(key)] = val\n",
    "\n",
    "# sort by popularity\n",
    "token_occurence = sorted(token_occurence, key=lambda key: token_occurence[val])\n",
    "\n",
    "#print top 10\n",
    "top10 = {k: token_occurence[k] for k in token_occurence.keys()[:10]}   \n",
    "    \n",
    "\n",
    "print top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zimin\t1\r\n",
      "zimin\t1\r\n",
      "zimin\t1\r\n",
      "zinc\t1\r\n",
      "zk\t1\r\n",
      "zo\t1\r\n",
      "zo\t1\r\n",
      "zolam\t1\r\n",
      "zolam\t1\r\n",
      "zxs\t1\r\n"
     ]
    }
   ],
   "source": [
    "#test mapper outside of Hadoop\n",
    "!cat enronemail_1h.txt | ./mapper.py | sort -k1,1 |tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1246\r\n",
      "to\t961\r\n",
      "and\t662\r\n",
      "of\t560\r\n",
      "you\t427\r\n",
      "in\t415\r\n",
      "your\t391\r\n",
      "for\t373\r\n",
      "this\t260\r\n",
      "on\t258\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | ./mapper.py | sort -k1,1 |./reducer.py |sort -k2,2nr | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "#test reducer outside of Hadoop\n",
    "!cat enronemail_1h.txt | ./mapper.py | sort -k1,1 |./reducer.py |grep \"assistance\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## HW 2.2.1 Using MapReduce on Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We let the Mapper and Reducer run on HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `HW2_2/output': No such file or directory\n",
      "16/06/04 18:06:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted enronemail_1h.txt\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar] /tmp/streamjob6977620921043626238.jar tmpDir=null\n",
      "16/06/04 18:06:55 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-7-251.us-west-1.compute.internal/172.31.7.251:8032\n",
      "16/06/04 18:06:55 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-7-251.us-west-1.compute.internal/172.31.7.251:8032\n",
      "16/06/04 18:06:56 INFO metrics.MetricsSaver: MetricsConfigRecord disabledInCluster: false instanceEngineCycleSec: 60 clusterEngineCycleSec: 60 disableClusterEngine: true maxMemoryMb: 3072 maxInstanceCount: 500 lastModified: 1464726748890 \n",
      "16/06/04 18:06:56 INFO metrics.MetricsSaver: Created MetricsSaver j-ZAC3GQDMC0E6:i-610a91d4:RunJar:27658 period:60 /mnt/var/em/raw/i-610a91d4_20160604_RunJar_27658_raw.bin\n",
      "16/06/04 18:06:56 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "16/06/04 18:06:56 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 426d94a07125cf9447bb0c2b336cf10b4c254375]\n",
      "16/06/04 18:06:56 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 18:06:56 INFO mapreduce.JobSubmitter: number of splits:16\n",
      "16/06/04 18:06:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464726740139_0032\n",
      "16/06/04 18:06:56 INFO impl.YarnClientImpl: Submitted application application_1464726740139_0032\n",
      "16/06/04 18:06:56 INFO mapreduce.Job: The url to track the job: http://ip-172-31-7-251.us-west-1.compute.internal:20888/proxy/application_1464726740139_0032/\n",
      "16/06/04 18:06:56 INFO mapreduce.Job: Running job: job_1464726740139_0032\n",
      "16/06/04 18:07:04 INFO mapreduce.Job: Job job_1464726740139_0032 running in uber mode : false\n",
      "16/06/04 18:07:04 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 18:07:14 INFO mapreduce.Job:  map 6% reduce 0%\n",
      "16/06/04 18:07:18 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "16/06/04 18:07:21 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/06/04 18:07:22 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "16/06/04 18:07:24 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "16/06/04 18:07:28 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "16/06/04 18:07:30 INFO mapreduce.Job:  map 81% reduce 0%\n",
      "16/06/04 18:07:31 INFO mapreduce.Job:  map 94% reduce 0%\n",
      "16/06/04 18:07:33 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 18:07:35 INFO mapreduce.Job:  map 100% reduce 43%\n",
      "16/06/04 18:07:36 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "16/06/04 18:07:38 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "16/06/04 18:07:40 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 18:07:40 INFO mapreduce.Job: Job job_1464726740139_0032 completed successfully\n",
      "16/06/04 18:07:40 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=68240\n",
      "\t\tFILE: Number of bytes written=3158893\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=910302\n",
      "\t\tHDFS: Number of bytes written=56904\n",
      "\t\tHDFS: Number of read operations=69\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=14\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=16\n",
      "\t\tLaunched reduce tasks=7\n",
      "\t\tData-local map tasks=8\n",
      "\t\tRack-local map tasks=8\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8114355\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5491530\n",
      "\t\tTotal time spent by all map tasks (ms)=180319\n",
      "\t\tTotal time spent by all reduce tasks (ms)=61017\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=180319\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=61017\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=259659360\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=175728960\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=30140\n",
      "\t\tMap output bytes=241665\n",
      "\t\tMap output materialized bytes=125254\n",
      "\t\tInput split bytes=2176\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5704\n",
      "\t\tReduce shuffle bytes=125254\n",
      "\t\tReduce input records=30140\n",
      "\t\tReduce output records=5704\n",
      "\t\tSpilled Records=60280\n",
      "\t\tShuffled Maps =112\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=112\n",
      "\t\tGC time elapsed (ms)=2863\n",
      "\t\tCPU time spent (ms)=34030\n",
      "\t\tPhysical memory (bytes) snapshot=8569446400\n",
      "\t\tVirtual memory (bytes) snapshot=56042278912\n",
      "\t\tTotal committed heap usage (bytes)=10174857216\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=908126\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=56904\n",
      "16/06/04 18:07:40 INFO streaming.StreamJob: Output directory: HW2_2/output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r HW2_2/output #Enron_email_reading_output\n",
    "!hdfs dfs -rm enronemail_1h.txt\n",
    "!hdfs dfs -copyFromLocal enronemail_1h.txt\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar \\\n",
    "    -files /home/hadoop/Notebooks/Users/Nina/mapper.py,/home/hadoop/Notebooks/Users/Nina/reducer.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input enronemail_1h.txt -output HW2_2/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\r\n",
      "-rw-r--r--   1 hadoop hadoop          0 2016-06-04 18:07 HW2_2/output/_SUCCESS\r\n",
      "-rw-r--r--   1 hadoop hadoop       8132 2016-06-04 18:07 HW2_2/output/part-00000\r\n",
      "-rw-r--r--   1 hadoop hadoop       7775 2016-06-04 18:07 HW2_2/output/part-00001\r\n",
      "-rw-r--r--   1 hadoop hadoop       7875 2016-06-04 18:07 HW2_2/output/part-00002\r\n",
      "-rw-r--r--   1 hadoop hadoop       8295 2016-06-04 18:07 HW2_2/output/part-00003\r\n",
      "-rw-r--r--   1 hadoop hadoop       8310 2016-06-04 18:07 HW2_2/output/part-00004\r\n",
      "-rw-r--r--   1 hadoop hadoop       8060 2016-06-04 18:07 HW2_2/output/part-00005\r\n",
      "-rw-r--r--   1 hadoop hadoop       8457 2016-06-04 18:07 HW2_2/output/part-00006\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls HW2_2/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we check it's output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000\t4\r\n",
      "0120\t1\r\n",
      "0344\t1\r\n",
      "036474336\t1\r\n",
      "0813\t1\r\n",
      "0841\t1\r\n",
      "093843\t1\r\n",
      "10000\t6\r\n",
      "100038\t1\r\n",
      "100foot\t1\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat HW2_2/output/part-00000 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we sort it by counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1246\r\n",
      "to\t961\r\n",
      "and\t662\r\n",
      "of\t560\r\n",
      "you\t427\r\n",
      "in\t415\r\n",
      "your\t391\r\n",
      "for\t373\r\n",
      "this\t260\r\n",
      "on\t258\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat HW2_2/output/part-*| sort -k2,2nr | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW 2.3 Multinomial Naive Bayes with No Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, we let our Mapper / Reducer learn a Multinomial Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Multinomial Naive Bayes needs to go through the training set of 100 emails, evaluate the frequency of each term in each category; and based on this, calculate the HAM/SPAM probability associated with each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_t.py\n",
    "#!/usr/bin/env python\n",
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    msg = line.strip().split('\\t', 2)\n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isSpam = msg[0], msg[1]\n",
    "    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    msgTxt = regex.sub(' ', msg[2].lower())\n",
    "    msgTxt = re.sub( '\\t', ' ', msgTxt )\n",
    "    msgTxt = re.sub( '\\s+', ' ', msgTxt )\n",
    "    # split the line into words\n",
    "    words = msgTxt.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        if len(word) >1: #drop single character words\n",
    "            print '%s\\t%d\\t%s\\t%s' % (word, 1, isSpam, msgID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_t.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "#from Decimal import *\n",
    "\n",
    "current_word = None\n",
    "smooth_factor = 0 # no smoothing\n",
    "current_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "\n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # handle msgID - store all IDs as we don't have too much\n",
    "    # not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "    \n",
    "    if word in wordcount.keys():\n",
    "        if isSpam ==0:\n",
    "            wordcount[word][0]+=1\n",
    "        if isSpam ==1:\n",
    "            wordcount[word][1]+=1\n",
    "    else:\n",
    "        if isSpam ==0:\n",
    "            wordcount[word]=[1,0]\n",
    "        if isSpam ==1:\n",
    "            wordcount[word]=[0,1]       \n",
    "\n",
    "\n",
    "### total count of all words in ham & spam:\n",
    "total_wc_ham = float(sum( wordcount[i][0] for i in wordcount.keys()))\n",
    "total_wc_spam = float(sum( wordcount[i][1] for i in wordcount.keys()))\n",
    "\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()): #/(1.0*n_total)):\n",
    "    word_total = value[0] + value[1]\n",
    "    p_word = float(word_total) / (total_wc_ham + total_wc_spam)\n",
    "    print key, \"%.9f\" % (p_word*value[0]*0.56/total_wc_ham), \"%.9f\" %(p_word*value[1]*0.44/total_wc_spam) #/(1.0*n_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_c.py\n",
    "#!/usr/bin/env python\n",
    "import sys, re, string, subprocess\n",
    "import sys, operator, math\n",
    "import numpy as np\n",
    "# read the probability from HDFS\n",
    "prob = {}\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"HW2_3/nbModel/part-*\"], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:    \n",
    "    word, p0, p1 = line.split()\n",
    "    prob[word] = [p0, p1]\n",
    "\n",
    "# get prior probability\n",
    "\n",
    "prior = [0.56, 0.44]\n",
    "\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    msg = line.split('\\t', 2)\n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, actualSPAMClass = msg[0], msg[1]\n",
    "    \n",
    "    msgTxt = regex.sub(' ', msg[2].lower())\n",
    "    msgTxt = re.sub( '\\t', ' ', msgTxt )\n",
    "    msgTxt = re.sub( '\\s+', ' ', msgTxt )\n",
    "    # split the line into words\n",
    "    words = msgTxt.split()\n",
    "    prHAMGivenDoc = math.log(float(prior[0]))\n",
    "    prSPAMGivenDoc = math.log(float(prior[1]))\n",
    "    for word in words:\n",
    "        if len(word) >1: #drop single letter words\n",
    "            if word in prob:\n",
    "                p0 = float(prob[word][0])\n",
    "                p1 = float(prob[word][1])\n",
    "                #print \"probs\", p0, p1, prHAMGivenDoc, prSPAMGivenDoc\n",
    "                wordGivenHam = math.log(p0) if p0>0.0 else float('-inf')\n",
    "                wordGivenSpam = math.log(p1) if p1>0.0 else float('-inf')\n",
    "                if wordGivenHam != 0.0 and prHAMGivenDoc != float('-inf'):\n",
    "                    prHAMGivenDoc = prHAMGivenDoc + wordGivenHam\n",
    "                else:\n",
    "                    prHAMGivenDoc = float('-inf')\n",
    "                if wordGivenSpam != 0.0 and prSPAMGivenDoc !=float('-inf'):   \n",
    "                    prSPAMGivenDoc = prSPAMGivenDoc + wordGivenSpam\n",
    "                else: \n",
    "                    prSPAMGivenDoc=float('-inf')\n",
    "            else: \n",
    "                print '%s\\t%s word not found in Multinomial Naive Bayes model lexicon' % (msgID, word)\n",
    "                sys.exit(\"error[\", word, \"]is not in the Multinomial Naive Bayes model lexicon\")\n",
    "    predictedClass = 1 #SPAM\n",
    "    if(prHAMGivenDoc > prSPAMGivenDoc):\n",
    "        predictedClass = 0 #HAM\n",
    "    if int(actualSPAMClass) == predictedClass:\n",
    "        print actualSPAMClass, predictedClass, prHAMGivenDoc, prSPAMGivenDoc,0  #no error\n",
    "    else: \n",
    "        print actualSPAMClass, predictedClass, prHAMGivenDoc, prSPAMGivenDoc,1 # error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_c.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator, math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import plotly.plotly as py\n",
    "#import plotly.graph_objs as go\n",
    "\n",
    "numberOfRecords = 0\n",
    "NumberOfMisclassifications=0\n",
    "prHAMGivenDoc = []\n",
    "prSPAMGivenDoc = []\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    #print line\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    toks = line.split(\" \")\n",
    "    # calculate the probabilities of HAM or SPAM of each email\n",
    "    prHAMGivenDoc.append(math.exp(float(toks[2])))\n",
    "    prSPAMGivenDoc.append(math.exp(float(toks[3])))\n",
    "    # account for the result\n",
    "    NumberOfMisclassifications = NumberOfMisclassifications + int(toks[4])\n",
    "    numberOfRecords = numberOfRecords + 1\n",
    "    \n",
    "# calculate the overall error rate\n",
    "# could also calcualte  the confusion matrix\n",
    "print 'Error rate: %.4f' %(1.0*NumberOfMisclassifications/float(numberOfRecords))\n",
    "print 'NumberOfMisclassifications %d, numberOfRecords%d'  %(NumberOfMisclassifications, numberOfRecords)\n",
    "\n",
    "\n",
    "prHAMGivenDoc = np.array(prHAMGivenDoc)\n",
    "prSPAMGivenDoc = np.array(prSPAMGivenDoc)\n",
    "plt.hist(prHAMGivenDoc, bins=50, color='blue')\n",
    "plt.hist(prSPAMGivenDoc, bins=50, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt|cut -f 2|grep 1 |wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t christmas tree farm pictures\tNA\r",
      "\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt|head -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these work well, they just had a long output\n",
    "\n",
    "!chmod a+x mapper_t.py \n",
    "!chmod a+x reducer_t.py \n",
    "\n",
    "#!cat enronemail_1h.txt|head -10 | ./mapper_t.py | ./reducer_t.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: write error: Broken pipe\n",
      "Error rate: 0.0000\n",
      "NumberOfMisclassifications 0, numberOfRecords15\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper_c.py \n",
    "!chmod a+x reducer_c.py \n",
    "\n",
    "!cat enronemail_1h.txt|head -15 | ./mapper_c.py | ./reducer_c.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat enronemail_1h.txt | ./mapper.py | sort -k1,1 |./reducer.py |grep \"assistance\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `HW2_3': File exists\n",
      "put: `HW2_3/enronemail_1h.txt': File exists\n",
      "16/06/07 03:14:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted HW2_3/nbModel\n",
      "16/06/07 03:14:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted HW2_3/classifications\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar] /tmp/streamjob2846924059786931467.jar tmpDir=null\n",
      "16/06/07 03:15:02 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-7-251.us-west-1.compute.internal/172.31.7.251:8032\n",
      "16/06/07 03:15:02 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-7-251.us-west-1.compute.internal/172.31.7.251:8032\n",
      "16/06/07 03:15:03 INFO metrics.MetricsSaver: MetricsConfigRecord disabledInCluster: false instanceEngineCycleSec: 60 clusterEngineCycleSec: 60 disableClusterEngine: true maxMemoryMb: 3072 maxInstanceCount: 500 lastModified: 1464726748890 \n",
      "16/06/07 03:15:03 INFO metrics.MetricsSaver: Created MetricsSaver j-ZAC3GQDMC0E6:i-610a91d4:RunJar:08357 period:60 /mnt/var/em/raw/i-610a91d4_20160607_RunJar_08357_raw.bin\n",
      "16/06/07 03:15:03 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "16/06/07 03:15:03 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 426d94a07125cf9447bb0c2b336cf10b4c254375]\n",
      "16/06/07 03:15:03 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/07 03:15:03 INFO mapreduce.JobSubmitter: number of splits:16\n",
      "16/06/07 03:15:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464726740139_0038\n",
      "16/06/07 03:15:04 INFO impl.YarnClientImpl: Submitted application application_1464726740139_0038\n",
      "16/06/07 03:15:04 INFO mapreduce.Job: The url to track the job: http://ip-172-31-7-251.us-west-1.compute.internal:20888/proxy/application_1464726740139_0038/\n",
      "16/06/07 03:15:04 INFO mapreduce.Job: Running job: job_1464726740139_0038\n",
      "16/06/07 03:15:12 INFO mapreduce.Job: Job job_1464726740139_0038 running in uber mode : false\n",
      "16/06/07 03:15:12 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/07 03:15:22 INFO mapreduce.Job:  map 6% reduce 0%\n",
      "16/06/07 03:15:25 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "16/06/07 03:15:29 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "16/06/07 03:15:33 INFO mapreduce.Job:  map 69% reduce 0%\n",
      "16/06/07 03:15:34 INFO mapreduce.Job:  map 81% reduce 0%\n",
      "16/06/07 03:15:37 INFO mapreduce.Job:  map 88% reduce 0%\n",
      "16/06/07 03:15:38 INFO mapreduce.Job:  map 94% reduce 0%\n",
      "16/06/07 03:15:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/07 03:15:42 INFO mapreduce.Job:  map 100% reduce 14%\n",
      "16/06/07 03:15:43 INFO mapreduce.Job:  map 100% reduce 43%\n",
      "16/06/07 03:15:44 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "16/06/07 03:15:45 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "16/06/07 03:15:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/07 03:15:47 INFO mapreduce.Job: Job job_1464726740139_0038 completed successfully\n",
      "16/06/07 03:15:47 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=213773\n",
      "\t\tFILE: Number of bytes written=3392849\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=910398\n",
      "\t\tHDFS: Number of bytes written=174848\n",
      "\t\tHDFS: Number of read operations=69\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=14\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=16\n",
      "\t\tLaunched reduce tasks=7\n",
      "\t\tData-local map tasks=8\n",
      "\t\tRack-local map tasks=8\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10319400\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5097420\n",
      "\t\tTotal time spent by all map tasks (ms)=229320\n",
      "\t\tTotal time spent by all reduce tasks (ms)=56638\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=229320\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=56638\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=330220800\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=163117440\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31413\n",
      "\t\tMap output bytes=1052045\n",
      "\t\tMap output materialized bytes=213044\n",
      "\t\tInput split bytes=2272\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5372\n",
      "\t\tReduce shuffle bytes=213044\n",
      "\t\tReduce input records=31413\n",
      "\t\tReduce output records=5372\n",
      "\t\tSpilled Records=62826\n",
      "\t\tShuffled Maps =112\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=112\n",
      "\t\tGC time elapsed (ms)=3711\n",
      "\t\tCPU time spent (ms)=35430\n",
      "\t\tPhysical memory (bytes) snapshot=8640503808\n",
      "\t\tVirtual memory (bytes) snapshot=55981715456\n",
      "\t\tTotal committed heap usage (bytes)=10237247488\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=908126\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=174848\n",
      "16/06/07 03:15:47 INFO streaming.StreamJob: Output directory: HW2_3/nbModel\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir HW2_3\n",
    "!hdfs dfs -put enronemail_1h.txt HW2_3\n",
    "!hdfs dfs -rm -r HW2_3/nbModel\n",
    "!hdfs dfs -rm -r HW2_3/classifications\n",
    "\n",
    "# Run MNB training job\n",
    "!hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar  \\\n",
    "    -files /home/hadoop/Notebooks/Users/Nina/mapper_t.py,/home/hadoop/Notebooks/Users/Nina/reducer_t.py \\\n",
    "    -mapper mapper_t.py \\\n",
    "    -reducer reducer_t.py \\\n",
    "    -input HW2_3/enronemail_1h.txt -output HW2_3/nbModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir HW2_3\n",
    "!hdfs dfs -put enronemail_1h.txt HW2_3\n",
    "#!hdfs dfs -rm -r HW2_3/nbModel\n",
    "!hdfs dfs -rm -r HW2_3/classifications\n",
    "\n",
    "# Run MNB training job\n",
    "!hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar  \\\n",
    "    -files /home/hadoop/Notebooks/Users/Nina/mapper_t.py,/home/hadoop/Notebooks/Users/Nina/reducer_t.py \\\n",
    "    -mapper mapper_c.py \\\n",
    "    -reducer reducer_c.py \\\n",
    "    -input HW2_3/enronemail_1h.txt -output HW2_3/classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liar 0.000000000 0.000000103\t\n",
      "chinese 0.000000000 0.000000026\t\n",
      "saying 0.000000000 0.000000414\t\n",
      "rob 0.000000000 0.000000233\t\n",
      "personally 0.000000000 0.000000026\t\n",
      "dollar 0.000000000 0.000000026\t\n",
      "focus 0.000000182 0.000000310\t\n",
      "krgp 0.000000182 0.000000000\t\n",
      "existing 0.000000684 0.000000259\t\n",
      "783518 0.000000000 0.000000026\t\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat HW2_3/nbModel/part-*|head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " (I have to admit I'm not sure why doesn't Matplotlib show the histogram; however, ) We can see that this classifier seems to perform extremely well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.4 Classification with Laplace plus-one smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the same process as in the previous exercise, just slightly change the formula in the reducer that evaluates posterior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_t.py\n",
    "#!/usr/bin/env python\n",
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    msg = line.strip().split('\\t', 2)\n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, isSpam = msg[0], msg[1]\n",
    "    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    msgTxt = regex.sub(' ', msg[2].lower())\n",
    "    msgTxt = re.sub( '\\t', ' ', msgTxt )\n",
    "    msgTxt = re.sub( '\\s+', ' ', msgTxt )\n",
    "    # split the line into words\n",
    "    words = msgTxt.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        if len(word) >1: #drop single character words\n",
    "            print '%s\\t%d\\t%s\\t%s' % (word, 1, isSpam, msgID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_t.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_t.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "#from Decimal import *\n",
    "\n",
    "current_word = None\n",
    "smooth_factor = 0 # no smoothing\n",
    "current_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "\n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # handle msgID - store all IDs as we don't have too much\n",
    "    # not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "    \n",
    "    if word in wordcount.keys():\n",
    "        if isSpam ==0:\n",
    "            wordcount[word][0]+=1\n",
    "        if isSpam ==1:\n",
    "            wordcount[word][1]+=1\n",
    "    else:\n",
    "        if isSpam ==0:\n",
    "            wordcount[word]=[1,0]\n",
    "        if isSpam ==1:\n",
    "            wordcount[word]=[0,1]       \n",
    "\n",
    "\n",
    "### total count of all words in ham & spam:\n",
    "total_wc_ham = float(sum( wordcount[i][0] for i in wordcount.keys()))\n",
    "total_wc_spam = float(sum( wordcount[i][1] for i in wordcount.keys()))\n",
    "\n",
    "## this section was change for Laplace Smoothing\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()): #/(1.0*n_total)):\n",
    "    word_total = value[0] + value[1]\n",
    "    p_word = float(word_total+1) / (total_wc_ham + total_wc_spam)\n",
    "    print key, \"%.9f\" % (p_word*value[0]*0.56/total_wc_ham), \"%.9f\" %(p_word*value[1]*0.44/total_wc_spam) #/(1.0*n_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_c.py\n",
    "#!/usr/bin/env python\n",
    "import sys, re, string, subprocess\n",
    "import sys, operator, math\n",
    "import numpy as np\n",
    "# read the probability from HDFS\n",
    "prob = {}\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"HW2_3/nbModel/part-*\"], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:    \n",
    "    word, p0, p1 = line.split()\n",
    "    prob[word] = [p0, p1]\n",
    "\n",
    "# get prior probability\n",
    "\n",
    "prior = [0.56, 0.44]\n",
    "\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # use subject and body\n",
    "    msg = line.split('\\t', 2)\n",
    "    if len(msg) < 3:\n",
    "        continue\n",
    "    msgID, actualSPAMClass = msg[0], msg[1]\n",
    "    \n",
    "    msgTxt = regex.sub(' ', msg[2].lower())\n",
    "    msgTxt = re.sub( '\\t', ' ', msgTxt )\n",
    "    msgTxt = re.sub( '\\s+', ' ', msgTxt )\n",
    "    # split the line into words\n",
    "    words = msgTxt.split()\n",
    "    prHAMGivenDoc = math.log(float(prior[0]))\n",
    "    prSPAMGivenDoc = math.log(float(prior[1]))\n",
    "    for word in words:\n",
    "        if len(word) >1: #drop single letter words\n",
    "            if word in prob:\n",
    "                p0 = float(prob[word][0])\n",
    "                p1 = float(prob[word][1])\n",
    "                #print \"probs\", p0, p1, prHAMGivenDoc, prSPAMGivenDoc\n",
    "                wordGivenHam = math.log(p0) if p0>0.0 else float('-inf')\n",
    "                wordGivenSpam = math.log(p1) if p1>0.0 else float('-inf')\n",
    "                if wordGivenHam != 0.0 and prHAMGivenDoc != float('-inf'):\n",
    "                    prHAMGivenDoc = prHAMGivenDoc + wordGivenHam\n",
    "                else:\n",
    "                    prHAMGivenDoc = float('-inf')\n",
    "                if wordGivenSpam != 0.0 and prSPAMGivenDoc !=float('-inf'):   \n",
    "                    prSPAMGivenDoc = prSPAMGivenDoc + wordGivenSpam\n",
    "                else: \n",
    "                    prSPAMGivenDoc=float('-inf')\n",
    "            else: \n",
    "                print '%s\\t%s word not found in Multinomial Naive Bayes model lexicon' % (msgID, word)\n",
    "                sys.exit(\"error[\", word, \"]is not in the Multinomial Naive Bayes model lexicon\")\n",
    "    predictedClass = 1 #SPAM\n",
    "    if(prHAMGivenDoc > prSPAMGivenDoc):\n",
    "        predictedClass = 0 #HAM\n",
    "    if int(actualSPAMClass) == predictedClass:\n",
    "        print actualSPAMClass, predictedClass, prHAMGivenDoc, prSPAMGivenDoc,0  #no error\n",
    "    else: \n",
    "        print actualSPAMClass, predictedClass, prHAMGivenDoc, prSPAMGivenDoc,1 # error\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_c.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys, operator, math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import plotly.plotly as py\n",
    "#import plotly.graph_objs as go\n",
    "\n",
    "numberOfRecords = 0\n",
    "NumberOfMisclassifications=0\n",
    "prHAMGivenDoc = []\n",
    "prSPAMGivenDoc = []\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    #print line\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    toks = line.split(\" \")\n",
    "    # calculate the probabilities of HAM or SPAM of each email\n",
    "    prHAMGivenDoc.append(math.exp(float(toks[2])))\n",
    "    prSPAMGivenDoc.append(math.exp(float(toks[3])))\n",
    "    # account for the result\n",
    "    NumberOfMisclassifications = NumberOfMisclassifications + int(toks[4])\n",
    "    numberOfRecords = numberOfRecords + 1\n",
    "    \n",
    "# calculate the overall error rate\n",
    "# could also calcualte  the confusion matrix\n",
    "print 'Error rate: %.4f' %(1.0*NumberOfMisclassifications/float(numberOfRecords))\n",
    "print 'NumberOfMisclassifications %d, numberOfRecords%d'  %(NumberOfMisclassifications, numberOfRecords)\n",
    "\n",
    "\n",
    "prHAMGivenDoc = np.array(prHAMGivenDoc)\n",
    "prSPAMGivenDoc = np.array(prSPAMGivenDoc)\n",
    "plt.hist(prHAMGivenDoc, bins=50, color='blue')\n",
    "plt.hist(prSPAMGivenDoc, bins=50, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these work well, they just had a long output\n",
    "\n",
    "!chmod a+x mapper_t.py \n",
    "!chmod a+x reducer_t.py \n",
    "\n",
    "#!cat enronemail_1h.txt|head -10 | ./mapper_t.py | ./reducer_t.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: write error: Broken pipe\n",
      "Error rate: 0.0000\n",
      "NumberOfMisclassifications 0, numberOfRecords15\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper_c.py \n",
    "!chmod a+x reducer_c.py \n",
    "\n",
    "!cat enronemail_1h.txt|head -15 | ./mapper_c.py | ./reducer_c.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir HW2_4\n",
    "!hdfs dfs -put enronemail_1h.txt HW2_4\n",
    "!hdfs dfs -rm -r HW2_4/nbModel\n",
    "!hdfs dfs -rm -r HW2_4/classifications\n",
    "\n",
    "# Run MNB training job\n",
    "!hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar  \\\n",
    "    -files /home/hadoop/Notebooks/Users/Nina/mapper_t.py,/home/hadoop/Notebooks/Users/Nina/reducer_t.py \\\n",
    "    -mapper mapper_t.py \\\n",
    "    -reducer reducer_t.py \\\n",
    "    -input HW2_4/enronemail_1h.txt -output HW2_4/nbModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r HW2_4/nbModel\n",
    "!hdfs dfs -rm -r HW2_4/classifications\n",
    "\n",
    "# Run MNB training job\n",
    "!hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar  \\\n",
    "    -files /home/hadoop/Notebooks/Users/Nina/mapper_t.py,/home/hadoop/Notebooks/Users/Nina/reducer_t.py \\\n",
    "    -mapper mapper_c.py \\\n",
    "    -reducer reducer_c.py \\\n",
    "    -input HW2_3/enronemail_1h.txt -output HW2_3/classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat HW2_3/classifications/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our results are consistent with teh previous exercise and with the previous homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.6 Benchmarking the results with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Multinomial Naive Bayes score: ', 98.0, '%')\n",
      "('Multinomial Naive Bayes error rate: ', 2.0000000000000018, '%')\n"
     ]
    }
   ],
   "source": [
    "categories = ['SPAM', 'HAM']\n",
    "\n",
    "docs = []\n",
    "labels = []\n",
    "\n",
    "with open('enronemail_1h.txt', 'r') as myfile:\n",
    "    for line in myfile:\n",
    "        labels.append(line.split(\"\\t\")[1])\n",
    "        docs.append(line.split(\"\\t\")[2] + line.split(\"\\t\")[3])\n",
    "\n",
    "docs=np.asarray(docs)\n",
    "labels=np.asarray(labels)\n",
    "\n",
    "\n",
    "#print 'labels shape:', labels.shape\n",
    "#print 'docs shape:', docs.shape\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2)\n",
    "data_vec = vectorizer.fit_transform(docs)\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(data_vec, labels)\n",
    "\n",
    "print ('Multinomial Naive Bayes score: ', 100 * MNB.score(data_vec,labels), '%' )\n",
    "print ('Multinomial Naive Bayes error rate: ', 100*(1-MNB.score(data_vec,labels)), '%'  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the default Multinomial Naive Bayes Algorithm from Scikit-Learn also classifies these emails with a 100% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bernoulli Naive Bayes score: ', 84.0, '%')\n",
      "('Bernoulli Naive Bayes error rate: ', 16.000000000000004, '%')\n"
     ]
    }
   ],
   "source": [
    "BNB = BernoulliNB()\n",
    "BNB.fit(data_vec, labels)\n",
    "\n",
    "print ('Bernoulli Naive Bayes score: ', 100 * BNB.score(data_vec,labels) , '%' )\n",
    "print ('Bernoulli Naive Bayes error rate: ', 100*(1-BNB.score(data_vec,labels)), '%'  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that the Bernoulli Naive Bayes Algorithm doesn't perform as well as the Multinomial Naive Bayes.\n",
    "\n",
    "One reason why the Multinomial Naive Bayes from scikit-learn doesn't perform as well as the one created with map-reduce is the smoothing. Without using the smoothing, even if a message has multiple features of spam, as long as it doesn't have one spam feature, its probability of being spam is evaluated as 0. However, our map-reduce-implemented classifier doesn't commit this mistake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
