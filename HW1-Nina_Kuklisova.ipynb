{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 0.0 - Bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nina Kuklišová is currently an Associate in Risk Analytics Group at Bank of Tokyo - Mitsubishi MUFG in New York. Prior to this, she worked as a Data Analyst in Bank of America, and as a Macroeconomic Quantitaive Researcher in Bloomberg. She has a Masters degree in Mathematics of Finance from Columbia University and a Bachelors degree in Mathematics and Physics from the University of Chicago. In college, she worked as a research assistant in High Energy Physics, and was the president of European Students Association. She was born and raised in Slovakia and got her high school Baccalauréat from a French Lycée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(('Nina Kuklišová is currently an Associate in Risk Analytics Group at Bank of Tokyo - Mitsubishi MUFG in New York. Prior to this, she worked as a Data Analyst in Bank of America, and as a Macroeconomic Quantitaive Researcher in Bloomberg. She has a Masters degree in Mathematics of Finance from Columbia University and a Bachelors degree in Mathematics and Physics from the University of Chicago. In college, she worked as a research assistant in High Energy Physics, and was the president of European Students Association. She was born and raised in Slovakia and got her high school Baccalauréat from a French Lycée.').split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 1.0.0 - Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Definition:$\n",
    "In general, Big data is a dataset that is so large or complex that it can't be stored and processed only on one personal computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Data is also characterized by big volume, variety, velocity, and possibly value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Example~from~the~domain~of~expertise~that~is~currently~earning~my~living:$\n",
    "\n",
    "Every day, traders at a bank do trades with various clients (counterparties). Using current and historical market data, we can estimate what is the current and expected value of each trade and risk on each position. Also, depending on the type of a client, we could use data to find some patterns in their trading strategy, or evaluate their credibility. We could also use data to detect insider trading, screen employees' communication to ensure politically correct and respectful work environment, and make sure that the financial institution complies with regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Example~from~my~previous~domain~of~expertise~-~High~Energy~Physics:$ \n",
    "\n",
    "We need to gather data from the entire particle detector, and use this to determine how did the special particles in this detector behave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 1.0.1 Bias Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to estimate the bias, the variance, the irreducible error, for a test dataset T when using polynomial regression of degree 1, 2, 3, 4, 5 is considered, and how is a model selected:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote the data points in the dataset T as $t_1$, $t_2$, ..., $t_n$; based on those, we estimated the parameters in our model. Then, if the fitted function was $ \\hat{f}(t)$, we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias: $\\mathbb{E} \\left(\\hat{f}(t_0) \\right) - f(t_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance: $\\mathbb{E} \\left[ \\hat{f}(t_0) - \\mathbb{E} \\hat{f}(t_0) \\right]^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irreducible error: $\\sigma_{\\epsilon}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $t_0$ is the point with the lowest $f(t)$, and $\\hat{f}(t)$ is the function that we're deciding about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the function was fitted, we evaluate the bias and variance with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias: $\\frac{\\sum_{i=1}^n ( \\hat{f} (t_i) - f(t_i) )}{n-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance: $\\frac{\\sum_{i=1}^n \\left( \\hat{f} (t_i) - f(t_i) \\right)^2}{n-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For each of our models $\\hat{f}$, we can let our algorithm calculate this bias and variance, from the $f(i)$ that it already has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, once this is calculated, we choose the model that minimizes the sum of these two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       8\r\n"
     ]
    }
   ],
   "source": [
    "!grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Nina Kuklisova\n",
    "## Description: mapper code for HW1.2-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "#count = 0\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[2] #\"enronemail_1h.txt\"\n",
    "findword = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    count = 0\n",
    "    for line in myfile.readlines(): \n",
    "        words = line.split() #= line.split(\"\\t\")[2] + line.split(\"\\t\")[3]\n",
    "        for word in words: #text.split(\" \"):\n",
    "            word = word.lower()\n",
    "            if word == \"assistance\":\n",
    "                count = count + 1\n",
    "        print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "print sys.argv\n",
    "sum = 0\n",
    "\n",
    "for filename in sys.argv[1:]:\n",
    "    \n",
    "    with open (filename, 'rb') as myfile:\n",
    "        for line in myfile:\n",
    "            sum = sum+int(line)\n",
    "            #print sum\n",
    "\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./reducer.py', 'enronemail_1h.txt.chunk.aa.counts']\r\n",
      "175\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 1 \"enronemail_1h.txt\"\n",
    "!cat *.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this means that the word 'assistance' occurs in this dataset 175 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.3 Classifying emails by occurrence of the word 'assistance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As inspired by the note to this exercise, we first need to find out how many words are there in spam messages in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Nina Kuklisova\n",
    "## Description: mapper code for HW1.2-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "#count = 0\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[2] #\"enronemail_1h.txt\"\n",
    "findword = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    count = 0\n",
    "    for line in myfile.readlines(): \n",
    "        words = line.split() \n",
    "        #we consider if the message is a spam or not\n",
    "        if words[1]==\"1\":\n",
    "            print len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "print sys.argv\n",
    "#sum = 0\n",
    "\n",
    "for filename in sys.argv[1:]:\n",
    "    sum = 0\n",
    "    \n",
    "    with open (filename, 'rb') as myfile:\n",
    "        for line in myfile:\n",
    "            #we count the line if the message was a spam\n",
    "            sum = sum+int(line)\n",
    "            #print sum\n",
    "\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./reducer.py', 'enronemail_1h.txt.chunk.aa.counts']\r\n",
      "19064\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 1 \"enronemail_1h.txt\"\n",
    "!cat *.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Which means that we have 19064 words in the spam messages in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how many of them contain the word 'assistance':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Nina Kuklisova\n",
    "## Description: mapper code for HW1.2-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "count = 0\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[2] #\"enronemail_1h.txt\"\n",
    "findword = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    #count = 0\n",
    "    for line in myfile.readlines(): \n",
    "        words = line.split() \n",
    "        #we consider if the message is a spam or not\n",
    "        if words[1]==\"1\":\n",
    "            for word in words[2:]:\n",
    "                if word == \"assistance\":\n",
    "                    count = count + 1\n",
    "        print count\n",
    "    #print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "print sys.argv\n",
    "#sum = 0\n",
    "\n",
    "for filename in sys.argv[1:]:\n",
    "    sum = 0\n",
    "    \n",
    "    with open (filename, 'rb') as myfile:\n",
    "        for line in myfile:\n",
    "            #we count the line if the message was a spam\n",
    "            sum = sum+int(line)\n",
    "            #print sum\n",
    "\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./reducer.py', 'enronemail_1h.txt.chunk.aa.counts']\r\n",
      "97\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 1 \"enronemail_1h.txt\"\n",
    "!cat *.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the term \"assistance\" occurs in spam messages 97 times, and therefore, probability that a message contains the word 'assistance' if it is a spam is just 97/19064."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Nina Kuklisova\n",
    "## Description: mapper code for HW1.2-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "#count = 0\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[2] #\"enronemail_1h.txt\"\n",
    "findword = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    count = 0\n",
    "    for line in myfile.readlines(): \n",
    "        words = line.split() \n",
    "        #we consider if the message is a spam or not\n",
    "        if words[1]==\"0\":\n",
    "            print len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "print sys.argv\n",
    "#sum = 0\n",
    "\n",
    "for filename in sys.argv[1:]:\n",
    "    sum = 0\n",
    "    \n",
    "    with open (filename, 'rb') as myfile:\n",
    "        for line in myfile:\n",
    "            #we count the line if the message was a spam\n",
    "            sum = sum+int(line)\n",
    "            #print sum\n",
    "\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./reducer.py', 'enronemail_1h.txt.chunk.aa.counts']\r\n",
      "14048\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 1 \"enronemail_1h.txt\"\n",
    "!cat *.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, there are 14048 words in non-spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Nina Kuklisova\n",
    "## Description: mapper code for HW1.2-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "#count = 0\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[2] #\"enronemail_1h.txt\"\n",
    "findword = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    count = 0\n",
    "    for line in myfile.readlines(): \n",
    "        words = line.split() \n",
    "        #we consider if the message is a spam or not\n",
    "        if words[1]==\"0\":\n",
    "            for word in words[2:]:\n",
    "                if word == \"assistance\":\n",
    "                    count = count + 1\n",
    "        print count\n",
    "    #print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "print sys.argv\n",
    "#sum = 0\n",
    "\n",
    "for filename in sys.argv[1:]:\n",
    "    sum = 0\n",
    "    \n",
    "    with open (filename, 'rb') as myfile:\n",
    "        for line in myfile:\n",
    "            #we count the line if the message was a spam\n",
    "            sum = sum+int(line)\n",
    "            #print sum\n",
    "\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./reducer.py', 'enronemail_1h.txt.chunk.aa.counts']\r\n",
      "78\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 1 \"enronemail_1h.txt\"\n",
    "!cat *.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And among those 14048 words in non-spam messages, we find the word 'assistance' 78 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use these findings for classifying our messages. Using only the words 'assistance', we can classify a message as spam if, based on our count of the words 'assistance' in it, we find:\n",
    "\n",
    "$ P(SPAM) > P(HAM)$.\n",
    "\n",
    "Based on our counts found above, we have:\n",
    "\n",
    "$\\begin{eqnarray*}\n",
    "P( SPAM~|~assistance) &= \\frac{P(assistance~|~SPAM)}{P(SPAM)} \\\\\n",
    "&=\\frac{ \\frac{97}{19064}}{ \\frac{44}{100}} \\\\\n",
    "&=\\frac{ 97 \\times 100}{19064 \\times 44}\\\\\n",
    "&\\tilde{=} 0.011563\n",
    "\\end{eqnarray*}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, very similarly:\n",
    "    \n",
    "$\\begin{eqnarray*}\n",
    "P( HAM~|~assistance) &= \\frac{P(assistance~|~HAM)}{P(HAM)} \\\\\n",
    "&=\\frac{ \\frac{78}{14048}}{ \\frac{56}{100}} \\\\\n",
    "&=\\frac{ 78 \\times 100}{14048 \\times 56}\\\\\n",
    "&\\tilde{=} 0.00991498\n",
    "\\end{eqnarray*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means that if a message contains the word 'assistance', it is more likely that it is a spam than if it did not contain that word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we have mapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Nina Kuklisova\n",
    "## Description: mapper code for HW1.2-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "#count = 0\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[2] #\"enronemail_1h.txt\"\n",
    "findword = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    count = 0\n",
    "    for line in myfile.readlines(): \n",
    "        words = line.split() \n",
    "        #we consider if the message is a spam or not\n",
    "        if \"assistance\" in words:\n",
    "            print 'SPAM'\n",
    "    #print count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW 1.4 Classifying emails by occurence of the words “assistance”, “valium”, and “enlargementWithATypo”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First of all, again, we need to find out how many times do these words occur in spam and non-spam email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "filename = sys.argv[1] #Takes a data chunk as an input\n",
    "findwords = sys.argv[2].lower() #List of words (in this case, a single word)\n",
    "key_words = findwords.split(\" \")\n",
    "key_words = [(key_words[i]).lower() for i in range(len(key_words))]\n",
    "for i in range(len(key_words)):\n",
    "    print key_words[i],\n",
    "print \"\\n\"\n",
    "\n",
    "\n",
    "#Searches each email (line) in the file for the user-specified word\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        #Creates a text field for SPAM/HAM from the binary field\n",
    "        ham_count = {key_words[i]:0 for i in range(len(key_words))}\n",
    "        spam_count = {key_words[i]:0 for i in range(len(key_words))}\n",
    "        words = line.split()\n",
    "        \n",
    "        #inspect words in spam:\n",
    "        if words[1]==\"0\":\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                if word in key_words:\n",
    "                    for key_word in key_words:\n",
    "                        if word == key_word:\n",
    "                            ham_count[key_word]+=1\n",
    "        #inspect words in non spam:\n",
    "        if words[1]==\"1\":\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                if word in key_words:\n",
    "                    for key_word in key_words:\n",
    "                        if word == key_word:\n",
    "                            spam_count[key_word]+=1\n",
    "\n",
    "        # for verification, we need to keep track of what is spam and what not\n",
    "        print words[1],            \n",
    "        for i in range(len(key_words)):\n",
    "            #print key_words[i]\n",
    "            print ham_count[key_words[i]], spam_count[key_words[i]],\n",
    "        print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance valium enlargementwithatypo \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 1 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 1 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 0 0 0 0 0 \r\n",
      "\r\n",
      "0 0 0 0 0 0 0 \r\n",
      "\r\n",
      "1 0 3 0 0 0 0 \r\n",
      "\r\n",
      "1 0 1 0 0 0 0 \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#test run for mapper\n",
    "\n",
    "! ./mapper.py \"enronemail_1h.txt\" \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "#For all countfiles from the mapper\n",
    "for f in sys.argv[1:]:\n",
    "    classified_correctly_ham = 0\n",
    "    classified_incorrectly_ham = 0\n",
    "    classified_correctly_spam = 0\n",
    "    classified_incorrectly_spam = 0\n",
    "    with open (f, \"r\") as myfile:\n",
    "        line_no = 0\n",
    "        for line in myfile:\n",
    "            words = line.split(\" \")\n",
    "            if line_no ==0:\n",
    "                key_words = words\n",
    "                counts = [0]*(2*len(key_words))\n",
    "                line_no +=1\n",
    "            else:\n",
    "                for i in range(len(counts)):\n",
    "                    counts[i]+=int(words[i+1])\n",
    "        ### now, we have a sum in format [word1_ham_count, word1_spam_count, word2_ham_count, ...]\n",
    "        \n",
    "        ### so, we can establish the probabilities that each given word occurs in the given email category:\n",
    "        \n",
    "        ham_spam_probs = [0]*len(counts)\n",
    "        for i in range(len(counts)):\n",
    "            if i %2 ==0:\n",
    "                ## probability that this word occurs in ham mail:\n",
    "                ham_spam_probs[i] = float(counts[i])/19064\n",
    "            else:\n",
    "                ## probability that this word occurs in spam mail:\n",
    "                ham_spam_probs[i] = float(counts[i])/14048\n",
    "        ### now, we have a summary [prob_word1_present_if_ham, prob_word1_present_if_spam, prob_word2_present_if_ham, ...]\n",
    "        \n",
    "        ### as we saw in office hours, the probability that the message is a spam, when it countains words x1, x2, ... is\n",
    "        ### P(SPAM | x1, x2, ... ) = P(SPAM) * P(x1|SPAM) * P(x2|SPAM) * ...\n",
    "                \n",
    "        ### and now, we can use a Naive Bayes Classifier:\n",
    "        p_ham_given_word = [0]*((len(counts)/2))\n",
    "        p_spam_given_word = [0]*((len(counts)/2))\n",
    "        \n",
    "        for line in myfile:\n",
    "            words = line.split(\" \")\n",
    "            p_ham = float(56)/100\n",
    "            p_spam = float(56)/100\n",
    "            if line_no > 1:\n",
    "                for i in range(len(counts)/2):\n",
    "                    word_count[i] = int(words[i]) + int(words[i+1])\n",
    "                    p_ham_given_word[i] = word_count[i] * ham_spam_probs[i]\n",
    "                    p_spam_given_word[i] = word_count[i] * ham_spam_probs[i+1]\n",
    "                    if word_count[i]>0:\n",
    "                        p_ham = p_ham*p_ham_given_word[i]\n",
    "                        p_spam = p_spam*p_spam_given_word[i]\n",
    "                ### announce the classifier's result and count the number of correctly\n",
    "                ### or incorrectly classified emails\n",
    "                if p_ham > p_spam:\n",
    "                    print \"HAM\"\n",
    "                    if words[0] == \"0\":\n",
    "                        classified_correctly_ham += 1\n",
    "                    else:\n",
    "                        classified_incorrectly_ham +=1                       \n",
    "                else:\n",
    "                    print \"SPAM\"\n",
    "                    if words[0] == \"1\":\n",
    "                        classified_correctly_spam += 1\n",
    "                    else:\n",
    "                        classified_incorrectly_spam +=1                       \n",
    "\n",
    "print ('Performance:')\n",
    "print ('Correctly classified: ', (classified_correctly_ham + classified_correctly_spam), '%')\n",
    "print ('Incorrectly classified: ',(classified_incorrectly_ham + classified_incorrectly_spam), '%')\n",
    "print ('Ham error rate: ', float(classified_incorrectly_ham)/56 )\n",
    "print ('Spam error rate: ', float(classified_incorrectly_spam)/56 )             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"./reducer.py\", line 22, in <module>\r\n",
      "    counts[i]+=int(words[i+1])\r\n",
      "IndexError: list index out of range\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 1 \"assistance valium enlargementWithATypo\"\n",
    "!cat *.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW 1.5 Classifying with Laplace smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The mapper that we want to use can be the same as the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "filename = sys.argv[1] \n",
    "findwords = sys.argv[2].lower() \n",
    "key_words = findwords.split(\" \")\n",
    "key_words = [(key_words[i]).lower() for i in range(len(key_words))]\n",
    "for i in range(len(key_words)):\n",
    "    print key_words[i],\n",
    "print \"\\n\"\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        #Creates a text field for SPAM/HAM from the binary field\n",
    "        ham_count = {key_words[i]:0 for i in range(len(key_words))}\n",
    "        spam_count = {key_words[i]:0 for i in range(len(key_words))}\n",
    "        words = line.split()\n",
    "        \n",
    "        #inspect words in spam:\n",
    "        if words[1]==\"0\":\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                if word in key_words:\n",
    "                    for key_word in key_words:\n",
    "                        if word == key_word:\n",
    "                            ham_count[key_word]+=1\n",
    "        #inspect words in non spam:\n",
    "        if words[1]==\"1\":\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                if word in key_words:\n",
    "                    for key_word in key_words:\n",
    "                        if word == key_word:\n",
    "                            spam_count[key_word]+=1\n",
    "\n",
    "        # for verification, we need to keep track of what is spam and what not\n",
    "        print words[1],            \n",
    "        for i in range(len(key_words)):\n",
    "            #print key_words[i]\n",
    "            print ham_count[key_words[i]], spam_count[key_words[i]],\n",
    "        print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in the reducer part, we need to change the calculation a little bit.\n",
    "As explained in more detail in the Wikipedia page, we classify a document as a spam if, knowing the words that it contains, it is more likely a spam than ham - that is, if\n",
    "$ \\frac{p(S|Doc)}{p(\\neg S|Doc)} = \\frac{p(S)}{p(\\neg S)} \\Pi_i{ \\frac{p(w_i | S)}{p(w_i| \\neg S)} }> 1$ .\n",
    "\n",
    "With Gaussian smoothing, we have:\n",
    "\n",
    "$p(w_i |S) = \\frac{ \\#~of~w_i~in~spam~documents +1}{\\#~of~all~words~in~spam~documents}$\n",
    "\n",
    "and\n",
    "\n",
    "$p(w_i | \\neg S) = \\frac{ \\#~of~w_i~in~non~spam~documents +1}{\\#~of~all~words~in~non~spam~documents}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "for f in sys.argv[1:]:\n",
    "    classified_correctly_ham = 0\n",
    "    classified_incorrectly_ham = 0\n",
    "    classified_correctly_spam = 0\n",
    "    classified_incorrectly_spam = 0\n",
    "    with open (f, \"r\") as myfile:\n",
    "        line_no = 0\n",
    "        for line in myfile:\n",
    "            words = line.split(\" \")\n",
    "            if line_no ==0:\n",
    "                key_words = words\n",
    "                counts = [0]*(2*len(key_words))\n",
    "                line_no +=1\n",
    "            else:\n",
    "                for i in range(len(counts)):\n",
    "                    counts[i]+=int(words[i+1])\n",
    "        ### now, we have a sum in format [word1_ham_count, word1_spam_count, word2_ham_count, ...]\n",
    "        \n",
    "        ### so, we can establish the probabilities that each given word occurs in the given email category:\n",
    "        \n",
    "        ham_spam_probs = [0]*len(counts)\n",
    "        for i in range(len(counts)):\n",
    "            if i %2 ==0:\n",
    "                ## probability that this word occurs in ham mail:\n",
    "                ham_spam_probs[i] = float(counts[i])/19064\n",
    "            else:\n",
    "                ## probability that this word occurs in spam mail:\n",
    "                ham_spam_probs[i] = float(counts[i])/14048\n",
    "        ### now, we have a summary [prob_word1_present_if_ham, prob_word1_present_if_spam, prob_word2_present_if_ham, ...]\n",
    "        \n",
    "        ### as we saw in office hours, the probability that the message is a spam, when it countains words x1, x2, ... is\n",
    "        ### P(SPAM | x1, x2, ... ) = P(SPAM) * P(x1|SPAM) * P(x2|SPAM) * ...\n",
    "                \n",
    "        ### and now, we can use a Naive Bayes Classifier:\n",
    "        p_ham_given_word = [0]*((len(counts)/2))\n",
    "        p_spam_given_word = [0]*((len(counts)/2))\n",
    "        \n",
    "        for line in myfile:\n",
    "            words = line.split(\" \")\n",
    "            p_ham = float(56)/100\n",
    "            p_spam = float(56)/100\n",
    "            if line_no > 1:\n",
    "                ### we changed this section for Gaussian Smoothing:\n",
    "                for i in range(len(counts)/2):\n",
    "                    word_count[i] = int(words[i]) + int(words[i+1])\n",
    "                    p_ham_given_word[i] = (word_count[i]+1) * ham_spam_probs[i]\n",
    "                    p_spam_given_word[i] = (word_count[i]+1) * ham_spam_probs[i+1]\n",
    "                    p_ham = p_ham*p_ham_given_word[i]\n",
    "                    p_spam = p_spam*p_spam_given_word[i]\n",
    "                ### announce the classifier's result and count the number of correctly\n",
    "                ### or incorrectly classified emails\n",
    "                if p_ham > p_spam:\n",
    "                    print \"HAM\"\n",
    "                    if words[0] == \"0\":\n",
    "                        classified_correctly_ham += 1\n",
    "                    else:\n",
    "                        classified_incorrectly_ham +=1                       \n",
    "                else:\n",
    "                    print \"SPAM\"\n",
    "                    if words[0] == \"1\":\n",
    "                        classified_correctly_spam += 1\n",
    "                    else:\n",
    "                        classified_incorrectly_spam +=1                       \n",
    "\n",
    "print ('Performance:')\n",
    "print ('Correctly classified: ', (classified_correctly_ham + classified_correctly_spam), '%')\n",
    "print ('Incorrectly classified: ',(classified_incorrectly_ham + classified_incorrectly_spam), '%')\n",
    "print ('Ham error rate: ', float(classified_incorrectly_ham)/56 )\n",
    "print ('Spam error rate: ', float(classified_incorrectly_spam)/56 )             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"./reducer.py\", line 21, in <module>\r\n",
      "    counts[i]+=int(words[i+1])\r\n",
      "IndexError: list index out of range\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 1 \"*\"\n",
    "!cat *.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Multinomial Naive Bayes returns 100% accuracy when it uses all words to classify, because this algorithm allows it to find words that are only present in spam emails (or, at least, more frequent there - for example, the word 'promotion' can be mentioned in both spam and ham email, but only in spam email, it would occur with term 'Viagra' or 'Nigeria')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW1.6 Benchmarking with Python scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Multinomial Naive Bayes score: ', 98.0, '%')\n",
      "('Multinomial Naive Bayes error rate: ', 2.0000000000000018, '%')\n"
     ]
    }
   ],
   "source": [
    "categories = ['SPAM', 'HAM']\n",
    "\n",
    "docs = []\n",
    "labels = []\n",
    "\n",
    "with open('enronemail_1h.txt', 'r') as myfile:\n",
    "    for line in myfile:\n",
    "        labels.append(line.split(\"\\t\")[1])\n",
    "        docs.append(line.split(\"\\t\")[2] + line.split(\"\\t\")[3])\n",
    "\n",
    "docs=np.asarray(docs)\n",
    "labels=np.asarray(labels)\n",
    "\n",
    "\n",
    "#print 'labels shape:', labels.shape\n",
    "#print 'docs shape:', docs.shape\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2)\n",
    "data_vec = vectorizer.fit_transform(docs)\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(data_vec, labels)\n",
    "\n",
    "print ('Multinomial Naive Bayes score: ', 100 * MNB.score(data_vec,labels), '%' )\n",
    "print ('Multinomial Naive Bayes error rate: ', 100*(1-MNB.score(data_vec,labels)), '%'  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So, the default Multinomial Naive Bayes Algorithm from Scikit-Learn also classifies these emails with a 100% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bernoulli Naive Bayes score: ', 84.0, '%')\n",
      "('Bernoulli Naive Bayes error rate: ', 16.000000000000004, '%')\n"
     ]
    }
   ],
   "source": [
    "BNB = BernoulliNB()\n",
    "BNB.fit(data_vec, labels)\n",
    "\n",
    "print ('Bernoulli Naive Bayes score: ', 100 * BNB.score(data_vec,labels) , '%' )\n",
    "print ('Bernoulli Naive Bayes error rate: ', 100*(1-BNB.score(data_vec,labels)), '%'  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here, we see that the Bernoulli Naive Bayes Algorithm doesn't perform as well as the Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One reason why the Multinomial Naive Bayes from scikit-learn doesn't perform as well as the one created with map-reduce is the smoothing. Without using the smoothing, even if a message has multiple features of spam, as long as it doesn't have one spam feature, its probability of being spam is evaluated as 0. However, our map-reduce-implemented classifier doesn't commit this mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW1.7 Bias - Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This homework happened to take me much more time than I initially expected, but if I had more time, I would try working with this dataset (https://catalog.data.gov/harvest/object/1d67945d-6758-4a68-acb9-1792a71f3b88/html) of US monthly tempaterature extremes. This is a dataset the weather conditions all around the US since 1886. The weather conditions are the minimum and maximum of daily temperature, precipitation and snowfall. In principle, these weather conditions should be a function of time of the year, altitude, latitude, and distance from ocean. However, it may be tempting to fit this dataset a more complicated function that minimal variation but shows some hidden parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
